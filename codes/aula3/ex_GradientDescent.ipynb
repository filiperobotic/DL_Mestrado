{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex_GradientDescent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filiperobotic/cursoDL/blob/master/codes/aula2/ex_GradientDescent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Hgv3Keqs4fh7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementando o Algoritmo Gradiente Descendente"
      ]
    },
    {
      "metadata": {
        "id": "50Pxs-C24fh-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Some helper functions for plotting and drawing lines\n",
        "\n",
        "def plot_points(X, y):\n",
        "    admitted = X[np.argwhere(y==1)]\n",
        "    rejected = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n",
        "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n",
        "\n",
        "def display(m, b, color='g--'):\n",
        "    plt.xlim(-0.05,1.05)\n",
        "    plt.ylim(-0.05,1.05)\n",
        "    x = np.arange(-10, 10, 0.1)\n",
        "    plt.plot(x, m*x+b, color)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KHNJCxwj4fiG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reading and plotting the data"
      ]
    },
    {
      "metadata": {
        "id": "QAAglG5J41jy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/filiperobotic/cursoDL.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wSifiRjk4fiI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('cursoDL/data.csv', header=None)\n",
        "X = np.array(data[[0,1]])\n",
        "y = np.array(data[2])\n",
        "plot_points(X,y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-3V4R4X4fid",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TODO: Implementar as Funções Básicas\n",
        "Agora é sua hora de brilhar! Implemente as seguintes fórmulas, conforme explicado no texto abaixo.\n",
        "- Função de ativação sigmoide\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "- Fórmula de predição (output)\n",
        "\n",
        "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
        "\n",
        "- Função de Erro\n",
        "\n",
        "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
        "\n",
        "- A função que atualiza os pesos\n",
        "\n",
        "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
        "\n",
        "$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$"
      ]
    },
    {
      "metadata": {
        "id": "x823Bhv94fif",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Implemente as seguintes funções\n",
        "\n",
        "#função de ativação (sigmoide). Obs: use np.exp para calcular o exponcial\n",
        "def sigmoid(x):\n",
        "    #TODO\n",
        "    pass\n",
        "\n",
        "# função de predição (output) \n",
        "def output_formula(features, weights, bias):\n",
        "    #TODO\n",
        "    return None\n",
        "\n",
        "# Fórmula de erro (log-loss). Obs: use np.log para calcular o logaritmo\n",
        "def error_formula(y, output):\n",
        "    #TODO\n",
        "    return None\n",
        "\n",
        "# Gradient descent step\n",
        "def update_weights(x, y, weights, bias, learnrate):\n",
        "    #TODO\n",
        "    return weights,bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RkK4c4Bs4fij",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Função de Treinamento\n",
        "Essa função irá ajudar a iterar o algoritmo de gradiente descendente por todos os dados, por um número de épocas. Ele também irá plotar os dados, assim como as linhas obtidas durante a execução do algoritmo."
      ]
    },
    {
      "metadata": {
        "id": "1TSXC-574fik",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(44)\n",
        "\n",
        "epochs = 100\n",
        "learnrate = 0.01\n",
        "\n",
        "def train(features, targets, epochs, learnrate, graph_lines=False):\n",
        "    \n",
        "    errors = []\n",
        "    n_records, n_features = features.shape\n",
        "    last_loss = None\n",
        "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "    bias = 0\n",
        "    for e in range(epochs):\n",
        "        del_w = np.zeros(weights.shape)\n",
        "        for x, y in zip(features, targets):\n",
        "            output = output_formula(x, weights, bias)\n",
        "            error = error_formula(y, output)\n",
        "            weights, bias = update_weights(x, y, weights, bias, learnrate)\n",
        "        \n",
        "        # Printing out the log-loss error on the training set\n",
        "        out = output_formula(features, weights, bias)\n",
        "        loss = np.mean(error_formula(targets, out))\n",
        "        errors.append(loss)\n",
        "        if e % (epochs / 10) == 0:\n",
        "            print(\"\\n========== Epoch\", e,\"==========\")\n",
        "            if last_loss and last_loss < loss:\n",
        "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "            else:\n",
        "                print(\"Train loss: \", loss)\n",
        "            last_loss = loss\n",
        "            predictions = out > 0.5\n",
        "            accuracy = np.mean(predictions == targets)\n",
        "            print(\"Accuracy: \", accuracy)\n",
        "        if graph_lines and e % (epochs / 100) == 0:\n",
        "            display(-weights[0]/weights[1], -bias/weights[1])\n",
        "            \n",
        "\n",
        "    # Plotting the solution boundary\n",
        "    plt.title(\"Solution boundary\")\n",
        "    display(-weights[0]/weights[1], -bias/weights[1], 'black')\n",
        "\n",
        "    # Plotting the data\n",
        "    plot_points(features, targets)\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting the error\n",
        "    plt.title(\"Error Plot\")\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.ylabel('Error')\n",
        "    plt.plot(errors)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e5rmdomM4fis",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(X, y, epochs, learnrate, True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}